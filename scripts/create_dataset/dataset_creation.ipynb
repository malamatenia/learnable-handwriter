{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05c158e4",
   "metadata": {},
   "source": [
    "# Learnable Handwriter Dataset Preparation\n",
    "\n",
    "This notebook transforms images and their associated ALTO XML files (typically the output from an automatic transcription platform like **eScriptorium**) into the format required to train the **Learnable Handwriter**.\n",
    "\n",
    "> ‚ö†Ô∏è **Note:**  \n",
    "> This notebook **cannot be run independently**.  \n",
    "> You need to download it and run it in a directory with a `data` folder containing:  \n",
    "> - an **`images`** folder with full manuscript or print images (`.jpeg`, `.jpg`, `.png`)  \n",
    "> - an **`annotations`** folder containing their corresponding ALTO XML annotation files\n",
    "\n",
    "Before you start, please read the [Learnable Handwriter Tutorial](https://learnable-handwriter.github.io/tutorial.html) if you haven‚Äôt done so already.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1c5e069",
   "metadata": {},
   "outputs": [],
   "source": [
    "#base imports\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "from pathlib import Path\n",
    "import os\n",
    "import rootutils\n",
    "from collections import defaultdict\n",
    "\n",
    "import json\n",
    "import csv\n",
    "import lxml\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819b25ab",
   "metadata": {},
   "source": [
    "### Define corpus paths  \n",
    "*(These are indicative ‚Äî adjust as needed)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3841be36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set this to the root directory of your dataset or project\n",
    "root_path = Path(\"/path/to/your/project/root\")  \n",
    "\n",
    "# Folder containing the raw ground truth data\n",
    "dataset_path = root_path / \"data\" / \"raw/ground/truth/folder/name\"\n",
    "\n",
    "# Folder with original uncropped images (change if your folder name differs)\n",
    "imgs_path = dataset_path / \"images\"\n",
    "\n",
    "# Optional: Path to your metadata CSV file (if using metadata)\n",
    "metadata_file_path = dataset_path / \"metadata/csv/name\"  \n",
    "\n",
    "# Folder with associated XML ALTO annotation files (change if your folder name differs)\n",
    "annotations_path = dataset_path / \"annotations\"\n",
    "\n",
    "# Folder where the processed cropped images will be saved\n",
    "img_save_path = root_path / \"datasets\" / \"processed/dataset/name\" / \"images\"\n",
    "\n",
    "# Path to save the final annotation JSON file\n",
    "annotations_json_path = root_path / \"datasets\" / \"processed/dataset/name\" / \"annotation.json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336b7a37",
   "metadata": {},
   "source": [
    "### Step 1: Match Images and Annotations\n",
    "\n",
    "- List all image filenames and all XML annotation filenames from their respective folders.\n",
    "- The XML annotation filenames **must exactly match** the image filenames (without the file extension), as is typically the case when exporting from tools like *eScriptorium*.\n",
    "- Identify any images missing their corresponding annotation files.\n",
    "- Create pairs of matched image and annotation filenames for further processing.\n",
    "- Print a quick summary and preview a few pairs to verify everything."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5317ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1Ô∏è‚É£ Get image filenames (e.g., 'btv1b53000323h_f474.jpg')\n",
    "# Path.rglob(\"*\") recursively lists all files in 'images' subfolders\n",
    "imgs_links = [f.name for f in imgs_path.rglob(\"*\") if f.is_file()]\n",
    "imgs = [f.stem for f in imgs_path.rglob(\"*\") if f.is_file()]  # same as imgs_links but without extension\n",
    "\n",
    "# 2Ô∏è‚É£ Get annotation filenames (e.g., 'Latin8236_f145.xml')\n",
    "files_links = [f.name for f in annotations_path.rglob(\"*\") if f.is_file()]\n",
    "annotation_names = [f.stem for f in annotations_path.rglob(\"*\") if f.is_file()]  # filenames without .xml\n",
    "\n",
    "# 3Ô∏è‚É£ Sanity check: find images with no matching annotation file\n",
    "missing_files = [img for img in imgs if f\"{img}.xml\" not in files_links]\n",
    "\n",
    "if missing_files:\n",
    "    print(\"‚ùå The following images do not have corresponding XML annotation files:\")\n",
    "    for m in missing_files:\n",
    "        print(f\"- {m}\")\n",
    "else:\n",
    "    print(\"‚úÖ All images have corresponding annotation files.\")\n",
    "\n",
    "# 4Ô∏è‚É£ Create valid (image, annotation) pairs\n",
    "image_file_pairs = [(img + \".jpg\", img + \".xml\") for img in imgs if f\"{img}.xml\" in files_links]\n",
    "\n",
    "print(f\"\\n‚úÖ Found {len(image_file_pairs)} valid image-annotation pairs.\")\n",
    "\n",
    "# 5Ô∏è‚É£ Preview some valid pairs\n",
    "for img, xml in image_file_pairs[:5]:\n",
    "    print(f\"{img} ‚ü∑ {xml}\")\n",
    "\n",
    "# 6Ô∏è‚É£ Print summary counts\n",
    "print(f\"\\nüì¶ Total image files found: {len(imgs_links)}\")\n",
    "print(f\"üóÇ Total annotation files found: {len(files_links)}\")\n",
    "\n",
    "# 7Ô∏è‚É£ Optionally inspect annotation file names (e.g., for parsing ark + folio)\n",
    "print(\"\\nüîç Annotation filenames (for ark/folio parsing):\")\n",
    "for filename in files_links[:5]:  # only show first 5 for brevity\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326ecf2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# üß± Create a dictionary to store the mapping between image names and their corresponding annotation filenames\n",
    "\n",
    "imgs_annotations = dict()\n",
    "\n",
    "for img in imgs:  # for all image base names (no extension)\n",
    "    for filename in files_links:  # check all annotation filenames\n",
    "        if img in filename:  # if the img ID is found in the annotation filename\n",
    "            imgs_annotations[f\"{img}.jpg\"] = filename  # map image to corresponding annotation\n",
    "\n",
    "# üßÆ Print stats\n",
    "print(f\"‚úÖ Created dictionary with {len(imgs_annotations)} image‚Äìannotation pairs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc398a1",
   "metadata": {},
   "source": [
    "### Step 2: Extract Text Lines with Transparent Backgrounds\n",
    "\n",
    "This script extracts individual text lines from document images using their ALTO XML annotations, applying an alpha channel to create transparent backgrounds around each cropped line.\n",
    "\n",
    "For each image, it:\n",
    "- Reads the corresponding ALTO XML annotation file.\n",
    "- Locates polygon coordinates of the text lines.\n",
    "- Creates cropped images with transparent backgrounds for each line.\n",
    "- Saves the cropped lines into folders named after the original images.\n",
    "- Stores the label for each line.\n",
    "\n",
    "‚è≥ Processing about 70 images typically takes around 12 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb30654c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup ---\n",
    "line_infos = dict()\n",
    "annotations = dict()\n",
    "lines_train = list()\n",
    "\n",
    "# Acceptable image formats\n",
    "img_extensions = ('.jpg', '.jpeg', '.png')\n",
    "\n",
    "def process_image(img_file):\n",
    "    annotation_file = imgs_annotations[img_file]\n",
    "    if annotation_file.lower() == \".ds_store\":\n",
    "        return 0, [], {}, {}\n",
    "\n",
    "    img_path = imgs_path / img_file\n",
    "    if not img_path.exists():\n",
    "        print(f\"Image file not found: {img_path}\")\n",
    "        return 0, [], {}, {}\n",
    "\n",
    "    img_folder_name = img_path.stem\n",
    "    img_folder_path = img_save_path / img_folder_name\n",
    "    if img_folder_path.exists():\n",
    "        print(f\"Folder {img_folder_path} already exists, skipping {img_file}.\")\n",
    "        return 0, [], {}, {}\n",
    "\n",
    "    img_folder_path.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"Saving extracted lines for {img_file} to: {img_folder_path}\")\n",
    "\n",
    "    img = cv2.imread(str(img_path))\n",
    "    if img is None:\n",
    "        print(f\"Error reading image: {img_path}\")\n",
    "        return 0, [], {}, {}\n",
    "\n",
    "    h, w, _ = img.shape\n",
    "\n",
    "    tree = ET.parse(annotations_path / annotation_file)\n",
    "    root = tree.getroot()\n",
    "    lines = root.findall(\n",
    "        \"{http://www.loc.gov/standards/alto/ns-v4#}Layout/\"\n",
    "        \"{http://www.loc.gov/standards/alto/ns-v4#}Page/\"\n",
    "        \"{http://www.loc.gov/standards/alto/ns-v4#}PrintSpace/\"\n",
    "        \"{http://www.loc.gov/standards/alto/ns-v4#}TextBlock/\"\n",
    "        \"{http://www.loc.gov/standards/alto/ns-v4#}TextLine\"\n",
    "    )\n",
    "\n",
    "    local_line_infos = {}\n",
    "    local_annotations = {}\n",
    "    missing_lines_local = []\n",
    "    count = 0\n",
    "\n",
    "    for line in lines:\n",
    "        id_line = line.get('ID')\n",
    "        line_name = f\"{img_path.stem}_{id_line}.png\"\n",
    "        label_tag = line.find('{http://www.loc.gov/standards/alto/ns-v4#}String')\n",
    "        if label_tag is None:\n",
    "            continue\n",
    "        label = label_tag.get('CONTENT')\n",
    "        local_line_infos[line_name] = {'label': label, 'page': img_file}\n",
    "\n",
    "        polygon_tag = line.find('{http://www.loc.gov/standards/alto/ns-v4#}Shape/{http://www.loc.gov/standards/alto/ns-v4#}Polygon')\n",
    "        if polygon_tag is None:\n",
    "            continue\n",
    "        polygon_str = polygon_tag.get('POINTS')\n",
    "        polygon_coords = [int(p) for p in polygon_str.strip().split()]\n",
    "        points = list(zip(polygon_coords[::2], polygon_coords[1::2]))\n",
    "\n",
    "        mask = np.zeros((h, w), dtype=np.uint8)\n",
    "        cv2.fillPoly(mask, [np.array(points, dtype=np.int32)], 255)\n",
    "        alpha = np.ones(img.shape[:2], dtype=np.uint8) * 255\n",
    "        alpha[mask == 0] = 0\n",
    "        masked_img = cv2.merge((img, alpha))\n",
    "\n",
    "        y_coords, x_coords = np.nonzero(mask)\n",
    "        if len(y_coords) == 0 or len(x_coords) == 0:\n",
    "            missing_lines_local.append(line_name)\n",
    "            continue\n",
    "\n",
    "        line_coords = {'ulx': min(x_coords), 'uly': min(y_coords), 'lrx': max(x_coords), 'lry': max(y_coords)}\n",
    "        line_img = masked_img[line_coords['uly']:line_coords['lry'], line_coords['ulx']:line_coords['lrx']]\n",
    "\n",
    "        line_img_path = img_folder_path / line_name\n",
    "        cv2.imwrite(str(line_img_path), line_img, [cv2.IMWRITE_PNG_COMPRESSION, 9])  # max compression for quality\n",
    "\n",
    "        split = 'train' if any(img_file.endswith(ext) for ext in img_extensions) else 'test'\n",
    "        local_annotations[line_name] = {'label': label, 'split': split}\n",
    "        count += 1\n",
    "\n",
    "        if not line_img_path.exists():\n",
    "            missing_lines_local.append(line_name)\n",
    "\n",
    "    return count, missing_lines_local, local_line_infos, local_annotations\n",
    "\n",
    "\n",
    "# --- Run in parallel ---\n",
    "num_workers = min(8, multiprocessing.cpu_count())  # limit workers to 8 or number of CPUs\n",
    "total_lines = 0\n",
    "all_missing_lines = []\n",
    "all_line_infos = {}\n",
    "all_annotations = {}\n",
    "\n",
    "with ThreadPoolExecutor(max_workers=num_workers) as executor:\n",
    "    futures = {executor.submit(process_image, img_file): img_file for img_file in imgs_annotations.keys()}\n",
    "    for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "        count, missing_lines_local, local_line_infos, local_annotations = future.result()\n",
    "        total_lines += count\n",
    "        all_missing_lines.extend(missing_lines_local)\n",
    "        all_line_infos.update(local_line_infos)\n",
    "        all_annotations.update(local_annotations)\n",
    "\n",
    "# Update global dicts after processing\n",
    "line_infos.update(all_line_infos)\n",
    "annotations.update(all_annotations)\n",
    "\n",
    "# --- Final summary ---\n",
    "print(f'Number of lines: {total_lines}')\n",
    "if all_missing_lines:\n",
    "    print(\"The following segmented images are missing:\", all_missing_lines)\n",
    "else:\n",
    "    print(\"All segmented images are present.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a3b316",
   "metadata": {},
   "source": [
    "### Step 3: Assign Dataset Splits\n",
    "\n",
    "By default, this step assigns `'split' = 'train'` to all labels.\n",
    "\n",
    "If you want to include validation (`'val'`) or other splits, you‚Äôll need to modify the code to define how those splits are assigned, for example by:\n",
    "- Random sampling\n",
    "- Specific groups or document types\n",
    "- Any other criteria that suit your dataset\n",
    "\n",
    "This allows you to customize the training and validation subsets for fine-tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb8ebf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_file in annotations.keys():\n",
    "    annotations[img_file]['split'] = 'train'\n",
    "\n",
    "with open(annotations_json_path, 'w') as f:\n",
    "    json.dump(annotations, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f3a727",
   "metadata": {},
   "source": [
    "### Sanity Check: Line Counts and Empty Lines\n",
    "\n",
    "Next, we calculate the number of text lines per image and check for any null (empty) lines.\n",
    "\n",
    "‚ö†Ô∏è **Important:** Null lines will **not** be skipped by the Learnable Handwriter and will cause training to fail.  \n",
    "Make sure to remove or fix any empty lines before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c3c811f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the annotations JSON file\n",
    "with open(annotations_json_path, 'r') as f:\n",
    "    annotations = json.load(f)\n",
    "\n",
    "# Group files by prefix (before first underscore)\n",
    "groups = defaultdict(list)\n",
    "for img_file in annotations.keys():\n",
    "    group_id = img_file.split('_')[0]\n",
    "    groups[group_id].append(img_file)\n",
    "\n",
    "# Assign 'train' split to all images in each group after shuffling\n",
    "for group_id, imgs in groups.items():\n",
    "    random.shuffle(imgs)\n",
    "    for img_file in imgs:\n",
    "        annotations[img_file]['split'] = 'train'\n",
    "\n",
    "# Save updated annotations once\n",
    "with open(annotations_json_path, 'w') as f:\n",
    "    json.dump(annotations, f, indent=4)\n",
    "\n",
    "# Create DataFrame from annotations for exploration\n",
    "df_lines = pd.DataFrame.from_dict(annotations, orient='index')\n",
    "\n",
    "# Check for missing labels\n",
    "null_lines = df_lines['label'].isnull().sum() if 'label' in df_lines else 0\n",
    "print(f\"{null_lines} out of {len(df_lines)} lines have null 'label' values.\")\n",
    "\n",
    "# Optional: Count lines per document (using first two parts of filename)\n",
    "key_counts = defaultdict(int)\n",
    "for filename in annotations.keys():\n",
    "    unique_key = '_'.join(filename.split('_')[:2])\n",
    "    key_counts[unique_key] += 1\n",
    "\n",
    "key_counts_df = pd.DataFrame(list(key_counts.items()), columns=['ID', 'Nb_Lines'])\n",
    "key_counts_df.to_csv('document_lines.csv', index=False)\n",
    "print(key_counts_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7051880",
   "metadata": {},
   "source": [
    "### Step 4: Optional Script to Add Metadata to `annotation.json`\n",
    "\n",
    "A `script` field is **mandatory** for fine-tuning a group of documents. This metadata must be present in the `annotation.json` file for the fine-tuning process to work correctly.\n",
    "\n",
    "To add metadata using this script, you need an external CSV file containing at least the following columns:\n",
    "\n",
    "- **ID**: The document ID, which corresponds to the image filename (without the extension).  \n",
    "  *Example:* `btv1b84472995_f141`\n",
    "\n",
    "- **Script**: The script type or group name (e.g., script type like *Textualis* or a hand like *Raoulet*), depending on your dataset.\n",
    "\n",
    "We assume that the CSV `ID` values and the image/XML filenames share a consistent format where the **first two parts of the ID (separated by underscores) exactly match the image and XML filenames** (minus their extensions).  \n",
    "For example, for the image `btv1b53000323h_f474.jpg`, the corresponding metadata ID would start with `btv1b53000323h_f474_...`.  \n",
    "The script extracts this prefix (the first two underscore-separated parts) to correctly link each metadata entry to its corresponding annotation.\n",
    "\n",
    "You can also include additional columns with any other metadata fields you want to add and adjust the script accordingly to load those fields into your JSON annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e999433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CSV metadata\n",
    "script_mapping = {}\n",
    "\n",
    "with open(metadata_file_path, 'r', encoding='utf-8') as csv_file:\n",
    "    csv_reader = csv.DictReader(csv_file, delimiter=';')\n",
    "    for row in csv_reader:\n",
    "        script_mapping[row['ID']] = row['Script']\n",
    "\n",
    "# Load JSON annotation data\n",
    "with open(annotations_json_path, 'r', encoding='utf-8') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Update JSON data with metadata\n",
    "for key, value in data.items():\n",
    "    denominator = '_'.join(key.split('_')[:2])  # Extract prefix (first two parts before underscore)\n",
    "    if denominator in script_mapping:\n",
    "        value['script'] = script_mapping[denominator]\n",
    "\n",
    "# Save the updated JSON\n",
    "with open(annotations_json_path, 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(data, json_file, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87402c73",
   "metadata": {},
   "source": [
    "You can now use your `datasets/name-of-your-dataset` folder to train the Learnable Handwriter!  \n",
    "\n",
    "To install and get started, follow the instructions in the [Learnable Handwriter README](https://github.com/malamatenia/learnable-handwriter/tree/main)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lhr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
